{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc1c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e474dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de042013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f8492a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7faad659",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f808856",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ecf4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184d5e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbd0ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c2dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ecb985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "modelsXB = []\n",
    "predsXB = []\n",
    "\n",
    "# gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "xgbr_params = {\n",
    "            'n_estimators':9999,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.05333221939055333,\n",
    "            'min_child_weight': 4,\n",
    "            'gamma': 5.301218558776368e-08,\n",
    "            'subsample': 0.41010429946197946,\n",
    "            'colsample_bytree': 0.8298539920447499,\n",
    "            'reg_alpha': 0.000517878113716743,\n",
    "            'reg_lambda': 0.00030121415155097723,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': CFG.SEED}\n",
    "\n",
    "config.update({key:val for key, val in xgbr_params.items() if key not in ['random_state', 'eval_metric', 'verbosity', 'objective', 'n_jobs']})\n",
    "wandb.init(project='S3E10', name='XGBoost', group='XGBoost', config=config)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgbr_params)\n",
    "    \n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 100, callbacks=[wandb.xgboost.WandbCallback(log_model=False)]\n",
    "         )\n",
    "    modelsXB.append(model)\n",
    "    predsXB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420acf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c29a1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "020800d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64e47854",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a8830e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c65b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fbad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6d06c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a370d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "229ca2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "modelsXB = []\n",
    "predsXB = []\n",
    "\n",
    "# gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "xgbr_params = {\n",
    "            'n_estimators':9999,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.05333221939055333,\n",
    "            'min_child_weight': 4,\n",
    "            'gamma': 5.301218558776368e-08,\n",
    "            'subsample': 0.41010429946197946,\n",
    "            'colsample_bytree': 0.8298539920447499,\n",
    "            'reg_alpha': 0.000517878113716743,\n",
    "            'reg_lambda': 0.00030121415155097723,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': CFG.SEED}\n",
    "\n",
    "config.update({key:val for key, val in xgbr_params.items() if key not in ['random_state', 'eval_metric', 'verbosity', 'objective', 'n_jobs']})\n",
    "wandb.init(project='S3E10', name='XGBoost', group='XGBoost', config=config)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgbr_params)\n",
    "    \n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 100, callbacks=[wandb.xgboost.WandbCallback(log_model=False)]\n",
    "         )\n",
    "    modelsXB.append(model)\n",
    "    predsXB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a228d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [model[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot_bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61050fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c664428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.7111551 , 0.16333364, 0.12551117], dtype=float32),\n",
      " array([0.71454674, 0.16249926, 0.12295397], dtype=float32),\n",
      " array([0.70543176, 0.17237453, 0.12219369], dtype=float32),\n",
      " array([0.72080547, 0.16008782, 0.11910668], dtype=float32),\n",
      " array([0.6976138 , 0.17023507, 0.13215113], dtype=float32),\n",
      " array([0.718605  , 0.16489132, 0.11650369], dtype=float32),\n",
      " array([0.71374947, 0.16561152, 0.12063902], dtype=float32),\n",
      " array([0.7162871 , 0.1660631 , 0.11764979], dtype=float32),\n",
      " array([0.69904864, 0.17194287, 0.12900849], dtype=float32)]"
     ]
    }
   ],
   "source": [
    "[modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4bc1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot_bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "761aaf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57cf8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ebdb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsLB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'LGBM_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "069e3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb.wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5829cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb.wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e06542b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e4f3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae633a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ec3b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsLB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'LGBM_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b54df464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Ridge, BayesianRidge\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f43301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e31ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e03f75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44132b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e88ad4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d944bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_lead': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impunity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a898a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impunity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f876dcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:slgal79n) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RandomForest</strong> at: <a href='https://wandb.ai/g-broughton/S3E10/runs/slgal79n' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/slgal79n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230318_232225-slgal79n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:slgal79n). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6373165e034dfc94ade7a3fc451b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666958229998272, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/S3E10/notebooks/wandb/run-20230318_232307-4uz18tlz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/S3E10/runs/4uz18tlz' target=\"_blank\">RandomForest</a></strong> to <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">https://wandb.ai/g-broughton/S3E10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/S3E10/runs/4uz18tlz' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/4uz18tlz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e73b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe2a1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(X_test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
