{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0466cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc04538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eaa5cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3524298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903be477",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31fb1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "778f5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3955f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fab8bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799c3e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b27fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faea34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f9c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc08b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "662da6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1b7bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d6168d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4015273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66281f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e04802",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00c14fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a7b3bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f07752b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318627a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03be96ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:czy6e4ut) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb193bf5a464ba9bcf59900d803dc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.068 MB of 3.068 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/binary_accuracy</td><td>▁▆▇▇█</td></tr><tr><td>epoch/epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▃▂▂▁</td></tr><tr><td>epoch/lr</td><td>▁▁▁▁▁</td></tr><tr><td>epoch/val_binary_accuracy</td><td>▁▄▅▅█</td></tr><tr><td>epoch/val_loss</td><td>█▄▃▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/binary_accuracy</td><td>0.98825</td></tr><tr><td>epoch/epoch</td><td>4</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.04384</td></tr><tr><td>epoch/lr</td><td>0.001</td></tr><tr><td>epoch/val_binary_accuracy</td><td>0.99023</td></tr><tr><td>epoch/val_loss</td><td>0.03489</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NNtest</strong> at: <a href='https://wandb.ai/g-broughton/S3E10/runs/czy6e4ut' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/czy6e4ut</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230320_152106-czy6e4ut/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:czy6e4ut). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a85c7fbb6464e3295b3ff169c22f4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668913266645782, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/S3E10/notebooks/wandb/run-20230320_152550-h59q3gsv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/S3E10/runs/h59q3gsv' target=\"_blank\">NN</a></strong> to <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">https://wandb.ai/g-broughton/S3E10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/S3E10/runs/h59q3gsv' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/h59q3gsv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0816b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X_data,y_data,n_splits):\n",
    "\n",
    "    def gen():\n",
    "        for train_index, test_index in KFold(n_splits).split(X_data):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,y_train,X_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))\n",
    "\n",
    "dataset=make_dataset(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88dc4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None))>"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4e8519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64c84603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d65a94d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b10eb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a4e5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2e1e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shard(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9fe2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4614be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c8c293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d1493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2efd0b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(permutations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7badd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b35b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a76c6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    print(train)\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79b41475",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1f8ae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = (x, y for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05aebc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84672c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(-1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "275382c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4496a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06a08a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate([(x, y) for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f69bb11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4574379",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc7e8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4e7b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(-1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca04489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a7f2aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2af6afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "852075d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f30df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShardDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e3e3248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23afbd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "702a1107",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c951d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "150ba6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56b1c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5ecd7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21ab56b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d4203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8aa5081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d5abd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff0f9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "daf148e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "795a36db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d405ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dce5894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ced5fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b89aecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    #WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "36cd4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38bb011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82f05e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds #.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
