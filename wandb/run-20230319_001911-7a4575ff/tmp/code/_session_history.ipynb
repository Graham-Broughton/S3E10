{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5085d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c983271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87076775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ee23df",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8419c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07afe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0351f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e889fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fb8912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd8aa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c38e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "modelsXB = []\n",
    "predsXB = []\n",
    "\n",
    "# gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "xgbr_params = {\n",
    "            'n_estimators':9999,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.05333221939055333,\n",
    "            'min_child_weight': 4,\n",
    "            'gamma': 5.301218558776368e-08,\n",
    "            'subsample': 0.41010429946197946,\n",
    "            'colsample_bytree': 0.8298539920447499,\n",
    "            'reg_alpha': 0.000517878113716743,\n",
    "            'reg_lambda': 0.00030121415155097723,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': CFG.SEED}\n",
    "\n",
    "config.update({key:val for key, val in xgbr_params.items() if key not in ['random_state', 'eval_metric', 'verbosity', 'objective', 'n_jobs']})\n",
    "wandb.init(project='S3E10', name='XGBoost', group='XGBoost', config=config)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgbr_params)\n",
    "    \n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 100, callbacks=[wandb.xgboost.WandbCallback(log_model=False)]\n",
    "         )\n",
    "    modelsXB.append(model)\n",
    "    predsXB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1718476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6487db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22929278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c71cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f07d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d87aa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb7b6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e4da4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b71108",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e426e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "modelsXB = []\n",
    "predsXB = []\n",
    "\n",
    "# gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "xgbr_params = {\n",
    "            'n_estimators':9999,\n",
    "            'max_depth': 4,\n",
    "            'learning_rate': 0.05333221939055333,\n",
    "            'min_child_weight': 4,\n",
    "            'gamma': 5.301218558776368e-08,\n",
    "            'subsample': 0.41010429946197946,\n",
    "            'colsample_bytree': 0.8298539920447499,\n",
    "            'reg_alpha': 0.000517878113716743,\n",
    "            'reg_lambda': 0.00030121415155097723,\n",
    "            'n_jobs': -1,\n",
    "            'objective': 'binary:logistic',\n",
    "            'verbosity': 0,\n",
    "            'eval_metric': 'logloss',\n",
    "            'random_state': CFG.SEED}\n",
    "\n",
    "config.update({key:val for key, val in xgbr_params.items() if key not in ['random_state', 'eval_metric', 'verbosity', 'objective', 'n_jobs']})\n",
    "wandb.init(project='S3E10', name='XGBoost', group='XGBoost', config=config)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgbr_params)\n",
    "    \n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 100, callbacks=[wandb.xgboost.WandbCallback(log_model=False)]\n",
    "         )\n",
    "    modelsXB.append(model)\n",
    "    predsXB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1295fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [model[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot_bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a53f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "[model[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fab12564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.7111551 , 0.16333364, 0.12551117], dtype=float32),\n",
      " array([0.71454674, 0.16249926, 0.12295397], dtype=float32),\n",
      " array([0.70543176, 0.17237453, 0.12219369], dtype=float32),\n",
      " array([0.72080547, 0.16008782, 0.11910668], dtype=float32),\n",
      " array([0.6976138 , 0.17023507, 0.13215113], dtype=float32),\n",
      " array([0.718605  , 0.16489132, 0.11650369], dtype=float32),\n",
      " array([0.71374947, 0.16561152, 0.12063902], dtype=float32),\n",
      " array([0.7162871 , 0.1660631 , 0.11764979], dtype=float32),\n",
      " array([0.69904864, 0.17194287, 0.12900849], dtype=float32)]"
     ]
    }
   ],
   "source": [
    "[modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10dcd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot_bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eab17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsXB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'XGBoost_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})\n",
    "# feature_df = pd.DataFrame(feature_importance, index=X.columns)\n",
    "# feature_df\n",
    "# wandb.Table\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 4))\n",
    "# sns.barplot(x=feature_df.values.squeeze(), y=feature_df.index,\n",
    "#             color=palette[-3], linestyle=\"-\", width=0.5, errorbar='sd',\n",
    "#             linewidth=0.5, edgecolor=\"black\", ax=ax)\n",
    "# ax.set_title('Feature Importance', fontdict={'fontweight': 'bold'})\n",
    "# ax.set(xlabel=None)\n",
    "\n",
    "# for s in ['right', 'top']:\n",
    "#     ax.spines[s].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4129163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75cdcd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsLB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'LGBM_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "296c5fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb.wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9de94d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb.wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cdc1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a80cdf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5268a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsLB = []\n",
    "predsLB = []\n",
    "\n",
    "lgbr_params = {\n",
    "            'n_estimators': 9999,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.00693702575527996,\n",
    "            'subsample': 0.20851841295589477,\n",
    "            'colsample_bytree': 0.5784778854092203, \n",
    "            'reg_alpha': 0.2622912287429849,\n",
    "            'reg_lambda': 2.8702494234117617e-08,\n",
    "            'objective': 'binary',\n",
    "            'metric': 'logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': CFG.SEED\n",
    "        }\n",
    "config = {} | {\n",
    "    key: value for key, value in lgbr_params.items() if key not in ['']\n",
    "}\n",
    "wandb.init(project='S3E10', name='LightGBM', group='LightGBM', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    model = lgbm.LGBMClassifier(**lgbr_params)\n",
    "\n",
    "    model.fit(X=X_train, y=y_train,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric = 'logloss',\n",
    "          early_stopping_rounds = CFG.XG_PATIENCE,\n",
    "          verbose = 150,\n",
    "          callbacks=[wandb_callback()]\n",
    "         )\n",
    "    modelsLB.append(model)\n",
    "    predsLB.append(model.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4909eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance =  [modelsLB[x].feature_importances_ for x in range(CFG.NFOLDS*CFG.REPEATS)]\n",
    "feature_importance = np.average(feature_importance,axis=0)\n",
    "data = [[label, value] for (label, value) in zip(X.columns, feature_importance)]\n",
    "\n",
    "table = wandb.Table(data=data, columns=['label', 'value'])\n",
    "wandb.log({'LGBM_Feature_Importance': wandb.plot.bar(table, 'label', 'value', title=\"Feature Importance\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10ea6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.io.formats.style import Styler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Ridge, BayesianRidge\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.data_processing.fi import get_fi\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dee35424",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "config['scaler'] = \"standard\"\n",
    "\n",
    "scaled_tr = pd.DataFrame(scaler.fit_transform(train.drop(\"Class\", axis=1)), columns=train.drop(\"Class\", axis=1).columns)\n",
    "scaled_tr[\"Class\"] = train[\"Class\"]\n",
    "\n",
    "scaled_orig = pd.DataFrame(scaler.fit_transform(orig.drop(\"Class\", axis=1)), columns=orig.drop(\"Class\", axis=1).columns)\n",
    "scaled_orig[\"Class\"] = orig[\"Class\"]\n",
    "\n",
    "scaled_tst = pd.DataFrame(scaler.transform(test), columns=test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32c5b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(df):\n",
    "    n_components = df.shape[1]\n",
    "    pca = PCA(n_components=n_components, random_state=CFG.SEED)\n",
    "    \n",
    "    components = pca.fit_transform(df)\n",
    "    components = pd.DataFrame(components, columns=[f'PC{i}' for i in range(n_components)])\n",
    "    components['Class'] = df['Class']\n",
    "    exp_var = pca.explained_variance_ratio_\n",
    "    exp_var_cumsum = np.cumsum(exp_var)\n",
    "    return components, exp_var, exp_var_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36b0605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "components, exp_var, exp_var_cumsum = get_n_components(scaled_tr)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var_cumsum,\n",
    "        name='Cumulative Explained Variance',\n",
    "        line=dict(color=palette[0], width=2),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=list(range(len(exp_var_cumsum))),\n",
    "        y=exp_var,\n",
    "        name='Explained Variance',\n",
    "        marker_color=palette[1],\n",
    "        width=0.7,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title='Principal Components', titlefont_size=20, tickfont_size=16),\n",
    "    yaxis=dict(title='Explained Variance', titlefont_size=20, tickfont_size=16),\n",
    "    height=500, width=1000, title_text='Explained Variance by Principal Components', title_x=0.5, titlefont_size=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b930fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(train, test, N):\n",
    "    pca = PCA(n_components=N, random_state=CFG.SEED)\n",
    "    X = pca.fit_transform(train.drop(\"Class\", axis=1))\n",
    "    X = pd.DataFrame(X, columns=[f'PC{i}' for i in range(N)])\n",
    "    y = train['Class']\n",
    "    return pca.transform(test), X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31b04687",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d796c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_lead': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impunity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c985dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impunity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b940fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a87c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "config['n_components'] = N\n",
    "\n",
    "X_test, X, y = pca_transform(scaled_tr, scaled_tst, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d194be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsRF = []\n",
    "predsRF = []\n",
    "\n",
    "params = {\n",
    "        'criterion': 'log_loss',\n",
    "        'n_jobs': -1,\n",
    "        'random_state': CFG.SEED,\n",
    "        'verbose': False,\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': None,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None,\n",
    "        'min_impurity_decrease': 0.0,\n",
    "        'bootstrap': True,\n",
    "        'max_samples': None\n",
    "    }\n",
    "\n",
    "config = {} | {\n",
    "    key:value for key, value in params.items() if key not in ['criterion', 'n_jobs', 'random_state', 'verbose', 'bootstrap']\n",
    "}\n",
    "wandb.init(project='S3E10', name='RandomForest', group='RandomForest', config=config, reinit=True)\n",
    "\n",
    "for train_index, test_index in k_fold.split(X, y):\n",
    "    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X=X_train, y=y_train)\n",
    "    yprobas = model.predict_proba(X_valid)\n",
    "    ypred = model.predict(X_valid)\n",
    "    score = log_loss(y_valid, ypred)\n",
    "    print(f'Score: {score}')\n",
    "    \n",
    "    modelsRF.append(model)\n",
    "    predsRF.append(model.predict_proba(X_test)[:, 1])\n",
    "    wandb.sklearn.plot_classifier(model, \n",
    "                               X_train, X_valid, \n",
    "                               y_train, y_valid,\n",
    "                               ypred, yprobas,\n",
    "                               [0, 1],\n",
    "                               is_binary=True, \n",
    "                               model_name='RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af3aa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a7aa44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26f3bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print(lr.__repr__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48a49764",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print(lr.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a6fbc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print(type(lr.__repr__()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b84d40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "print(lr.__repr__()[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "551b1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, test, model, params):\n",
    "    models = []\n",
    "    preds = []\n",
    "\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = model(**params)\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        yprobas = model.predict_proba(X_valid)\n",
    "        ypred = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, ypred)\n",
    "        print(f'Score: {score}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preds.append(model.predict_proba(test)[:, 1])\n",
    "        wandb.sklearn.plot_classifier(model, \n",
    "                                X_train, X_valid, \n",
    "                                y_train, y_valid,\n",
    "                                ypred, yprobas,\n",
    "                                [0, 1],\n",
    "                                is_binary=True, \n",
    "                                model_name=model.__repr()[:-2])\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daf5d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, LogisticRegression, lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ad3dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, test, model, params):\n",
    "    models = []\n",
    "    preds = []\n",
    "\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = model(**params)\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        yprobas = model.predict_proba(X_valid)\n",
    "        ypred = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, ypred)\n",
    "        print(f'Score: {score}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preds.append(model.predict_proba(test)[:, 1])\n",
    "        wandb.sklearn.plot_classifier(model, \n",
    "                                X_train, X_valid, \n",
    "                                y_train, y_valid,\n",
    "                                ypred, yprobas,\n",
    "                                [0, 1],\n",
    "                                is_binary=True, \n",
    "                                model_name=model().__repr()[:-2])\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ba54228",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, LogisticRegression, lr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "69591215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, test, model):\n",
    "    models = []\n",
    "    preds = []\n",
    "\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #model = model(**params)\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        yprobas = model.predict_proba(X_valid)\n",
    "        ypred = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, ypred)\n",
    "        print(f'Score: {score}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preds.append(model.predict_proba(test)[:, 1])\n",
    "        wandb.sklearn.plot_classifier(model, \n",
    "                                X_train, X_valid, \n",
    "                                y_train, y_valid,\n",
    "                                ypred, yprobas,\n",
    "                                [0, 1],\n",
    "                                is_binary=True, \n",
    "                                model_name=model().__repr()[:-2])\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "029b141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, LogisticRegression(**lr_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c5473a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "lr = LogisticRegression(**lr_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54ec98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, test, model):\n",
    "    models = []\n",
    "    preds = []\n",
    "\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #model = model(**params)\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        yprobas = model.predict_proba(X_valid)\n",
    "        ypred = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, ypred)\n",
    "        print(f'Score: {score}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preds.append(model.predict_proba(test)[:, 1])\n",
    "        wandb.sklearn.plot_classifier(model, \n",
    "                                X_train, X_valid, \n",
    "                                y_train, y_valid,\n",
    "                                ypred, yprobas,\n",
    "                                [0, 1],\n",
    "                                is_binary=True, \n",
    "                                model_name=model.__repr()[:-2])\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb83073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "lr = LogisticRegression(**lr_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e22012c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseEstimator.__repr__ of LogisticRegression(C=0.1, max_iter=10000, n_jobs=-1, random_state=69, tol=0.001)>"
     ]
    }
   ],
   "source": [
    "lr.__repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95c279c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr().__repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "158225e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'LogisticRegression(C=0.1, max_iter=10000, n_jobs=-1, random_state=69, tol=0.001)'"
     ]
    }
   ],
   "source": [
    "str(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c39355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "print(lr.__repr__()[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1775807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "print(lr.__repr__()[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27bde1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LogisticRegression()\n",
    "print(lr.__repr__()[:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c2d96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, test, model, name):\n",
    "    models = []\n",
    "    preds = []\n",
    "\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #model = model(**params)\n",
    "        model.fit(X=X_train, y=y_train)\n",
    "        yprobas = model.predict_proba(X_valid)\n",
    "        ypred = model.predict(X_valid)\n",
    "        score = log_loss(y_valid, ypred)\n",
    "        print(f'Score: {score}')\n",
    "        \n",
    "        models.append(model)\n",
    "        preds.append(model.predict_proba(test)[:, 1])\n",
    "        wandb.sklearn.plot_classifier(model, \n",
    "                                X_train, X_valid, \n",
    "                                y_train, y_valid,\n",
    "                                ypred, yprobas,\n",
    "                                [0, 1],\n",
    "                                is_binary=True, \n",
    "                                model_name=name)\n",
    "    return models, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bcab4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "lr = LogisticRegression(**lr_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f6a8361",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "wandb.init(project='S3E10', name='LogisticRegression', group='LogisticRegression', config=lr_params, reinit=True)\n",
    "lr = LogisticRegression(**lr_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d20939c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'alpha': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "wandb.init(project='S3E10', name='RidgeRegression', group='RidgeRegression', config=r_params, reinit=True)\n",
    "lr = Ridge(**r_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr, 'RidgeRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "486befe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'alpha': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "wandb.init(project='S3E10', name='RidgeRegression', group='RidgeRegression', config=r_params, reinit=True)\n",
    "r = Ridge(**r_params)\n",
    "modelsR, predsR = fit_linear(X, y, X_test, r, 'RidgeRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2ada9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'alpha': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "wandb.init(project='S3E10', name='RidgeClassifier', group='RidgeClassifier', config=r_params, reinit=True)\n",
    "r = RidgeClassifier(**r_params)\n",
    "modelsR, predsR = fit_linear(X, y, X_test, r, 'RidgeClassifier')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "813048da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ffeahe9j) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">RidgeClassifier</strong> at: <a href='https://wandb.ai/g-broughton/S3E10/runs/ffeahe9j' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/ffeahe9j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230319_001852-ffeahe9j/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ffeahe9j). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f5347fede4d9d81ead5fe722da9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669899633355575, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/S3E10/notebooks/wandb/run-20230319_001911-7a4575ff</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/S3E10/runs/7a4575ff' target=\"_blank\">LogisticRegression</a></strong> to <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">https://wandb.ai/g-broughton/S3E10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/S3E10/runs/7a4575ff' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/7a4575ff</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_params = {\n",
    "    'random_state': CFG.SEED,\n",
    "    'max_iter': 10000,\n",
    "    'n_jobs': -1,\n",
    "    'solver': 'lbfgs',\n",
    "    'C': 0.1,\n",
    "    'tol': 1e-3\n",
    "}\n",
    "wandb.init(project='S3E10', name='LogisticRegression', group='LogisticRegression', config=lr_params, reinit=True)\n",
    "lr = LogisticRegression(**lr_params)\n",
    "modelsLR, predsLR = fit_linear(X, y, X_test, lr, 'LogisticRegression')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
