{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef282af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da26c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a1f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c330ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c0b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7072c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c0b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ce43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "103c8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7fa42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293612f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54978dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d18a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01c20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be6097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05836a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8bce7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f0dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00797dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fb23f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "599127b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e749b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c84b322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86c14ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed4189e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d79ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X_data,y_data,n_splits):\n",
    "\n",
    "    def gen():\n",
    "        for train_index, test_index in KFold(n_splits).split(X_data):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,y_train,X_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))\n",
    "\n",
    "dataset=make_dataset(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54174ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None))>"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64da1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5dcd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "761200b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e9b5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04caf592",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d03481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shard(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99dcb58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81d22d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fa26ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3deba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8677f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(permutations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68807f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "118ef8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a90b031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    print(train)\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d1a5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "659053cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = (x, y for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b2876a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5569595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(-1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b63bb715",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b566d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978ca964",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate([(x, y) for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb73c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4791b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "032196d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a37c9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(-1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59f4a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f23ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0e0b34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a925ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8be249ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShardDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5eb059a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ec52d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "15c5b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6b01247",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4805ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a70ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "302e47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68bc4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15807d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8402108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a94756e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73427bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be4da09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f8b39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7041e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59ff3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d41b0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7c5be4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    #WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "399444b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da7e9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "997f923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds #.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5f411744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:urou0101) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800c9106831e44379d36d31b0de6c25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.043 MB of 0.047 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.911670…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NNtest</strong> at: <a href='https://wandb.ai/g-broughton/S3E10/runs/urou0101' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/urou0101</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230320_190432-urou0101/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:urou0101). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a400a4c19ba410b9efd491875fb744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666887461663767, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/S3E10/notebooks/wandb/run-20230320_190519-oq9wkmlb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/S3E10/runs/oq9wkmlb' target=\"_blank\">NNtest</a></strong> to <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">https://wandb.ai/g-broughton/S3E10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/S3E10/runs/oq9wkmlb' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/oq9wkmlb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a734083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params, tags=['CV5'])\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
