{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import catboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import cudf\n",
    "from cuml.ensemble import RandomForestClassifier as curfc\n",
    "from cuml.metrics import log_loss\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_eng(df):\n",
    "    df['Skewness_Power3'] = df['Skewness'].apply(lambda x: x**3)\n",
    "    df['EK_Power3'] = df['EK'].apply(lambda x: x**3)\n",
    "    df['cos(EK)'] = df['EK'].apply(lambda x: np.cos(x))\n",
    "    df['SD_x_EK'] = df['SD'] * df['EK']\n",
    "    df['cos(EK)_x_SD'] = df['cos(EK)'] * df['SD']\n",
    "    df['SD_DMSNR_Curve_x_Skewness_Power3'] = df['SD_DMSNR_Curve'] * df['Skewness_Power3']\n",
    "    df['EK_divide_EK_Power3'] = df['EK'] / df['EK_Power3']\n",
    "    df['EK_multiply_SD'] = df['EK'] * df['SD']\n",
    "    df['EK_divide_SD'] = df['EK'] / df['SD']\n",
    "    df['EK_multiply_SD_DMSNR_Curve'] = df['EK'] * df['SD_DMSNR_Curve']\n",
    "    df['SD_DMSNR_Curve_divide_SD'] = df['SD_DMSNR_Curve'] / df['SD']\n",
    "    df['SD_multiply_EK_Power3'] = df['SD'] * df['EK_Power3']\n",
    "    df['SD_DMSNR_Curve_multiply_SD'] = df['SD_DMSNR_Curve'] * df['SD']\n",
    "    df['EK_Power3_multiply_SD_DMSNR_Curve'] = df['EK_Power3'] * df['SD_DMSNR_Curve']\n",
    "    df['cos(EK)_multiply_SD_DMSNR_Curve'] = df['cos(EK)'] * df['SD_DMSNR_Curve']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = f_eng(train)\n",
    "test = f_eng(test)\n",
    "orig = f_eng(orig)\n",
    "\n",
    "y = train['Class']\n",
    "X = train.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_objective(trial):\n",
    "    PATIENCE = 100\n",
    "    depth = trial.suggest_int('depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    l2_leaf_reg = trial.suggest_loguniform('l2_leaf_reg', 1, 100)\n",
    "    random_strength = trial.suggest_loguniform('random_strength', 1, 100)\n",
    "    max_bin = trial.suggest_int('max_bin', 2, 255)\n",
    "    od_wait = trial.suggest_int('od_wait', 10, 100)\n",
    "    grow_policy = trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "    \n",
    "    gpu_params = {'task_type' : \"GPU\", 'devices' : '0'}\n",
    "    params = {\n",
    "        'depth': depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'l2_leaf_reg': l2_leaf_reg,\n",
    "        'random_strength': random_strength,\n",
    "        'max_bin': max_bin,\n",
    "        'od_wait': od_wait,\n",
    "        'grow_policy': grow_policy,\n",
    "        }\n",
    "    predsCB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = catboost.CatBoostClassifier(**params, **gpu_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose=False,\n",
    "            )\n",
    "        \n",
    "        predsCB.append(model.predict_proba(X)[:, 1])\n",
    "    resCB = np.mean(predsCB, axis=0)\n",
    "    \n",
    "    return log_loss(y, resCB)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(cb_objective, n_trials=50)\n",
    "CB_params = study.best_params\n",
    "CB_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xb_objective(trial):\n",
    "    PATIENCE = CFG.XB_PATIENCE\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "    subsample = trial.suggest_discrete_uniform('subsample', 0.4, 1.0, 0.1)\n",
    "    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.4, 1.0, 0.1)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 1e-8, 10.0)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 1e-8, 10.0)\n",
    "    \n",
    "    gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "    params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'gamma': gamma,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "    }\n",
    "    \n",
    "    xg_params = {\n",
    "        'n_jobs': -1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': CFG.SEED\n",
    "    }\n",
    "    \n",
    "    predsXB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = XGBClassifier(**params, **gpu_params, **xg_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose = False\n",
    "            )\n",
    "        \n",
    "        predsXB.append(model.predict_proba(X)[:, 1])\n",
    "    resXB = np.mean(predsXB, axis=0)\n",
    "    \n",
    "    return log_loss(y_valid, resXB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(xb_objective, n_trials=50)\n",
    "xgb_params = study.best_params\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lb_objective(trial):\n",
    "    PATIENCE = CFG.XB_PATIENCE\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    subsample = trial.suggest_discrete_uniform('subsample', 0.4, 1.0, 0.1)\n",
    "    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.4, 1.0, 0.1)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 1e-8, 10.0)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 1e-8, 10.0)\n",
    "    \n",
    "    # gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "    params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "    }\n",
    "\n",
    "    lg_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': CFG.SEED\n",
    "    }\n",
    "    \n",
    "    k_fold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=CFG.SEED) \n",
    "    predsLB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = lgbm.LGBMClassifier(**params, **lg_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric = 'logloss',\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose = False\n",
    "            )\n",
    "        \n",
    "        predsLB.append(model.predict_proba(X_valid)[:, 1])\n",
    "    resLB = np.mean(predsLB, axis=0)\n",
    "    \n",
    "    return log_loss(y_valid, resLB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lb_objective, n_trials=50)\n",
    "lb_params = study.best_params\n",
    "lb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_tr, y_tr, X_val, y_val, params, r_params):\n",
    "    cX = cudf.DataFrame.from_pandas(X)\n",
    "    cy = cudf.Series(y.values)\n",
    "\n",
    "    \n",
    "    model = curfc(**params, **r_params)\n",
    "    \n",
    "    model.fit(X=X_tr, y=y_tr)\n",
    "    preds = model.predict_proba(X_val)\n",
    "    loss = log_loss(y_val, preds)\n",
    "    return loss\n",
    "\n",
    "@wrapper\n",
    "def rf_obj(trial):\n",
    "    n_bins = trial.suggest_int('n_bins', 8, 256)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_uniform('max_features', 0.1, 1.0)\n",
    "    max_leaves = trial.suggest_int('max_leaf_nodes', 2, 30)\n",
    "    min_impurity_decrease = trial.suggest_uniform('min_impurity_decrease', 0.0, 0.5)\n",
    "    max_samples = trial.suggest_uniform('max_samples', 0.1, 1.0)\n",
    "    \n",
    "    params = {\n",
    "        'n_bins': n_bins,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_features': max_features,\n",
    "        'max_leaves': max_leaves,\n",
    "        'min_impurity_decrease': min_impurity_decrease,\n",
    "        'max_samples': max_samples,\n",
    "    }\n",
    "    r_params = {\n",
    "        'random_state': CFG.SEED,\n",
    "    }\n",
    "\n",
    "    model = curfc(**params, **r_params)\n",
    "    model.fit(X=X_tr, y=y_tr)\n",
    "    \n",
    "    preds = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, preds)\n",
    "\n",
    "def rf_objective(trial, X, y):\n",
    "    \n",
    "    losses = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_valid = cudf.DataFrame.from_pandas(X_train), cudf.DataFrame.from_pandas(X_valid)\n",
    "        y_train, y_valid = cudf.Series(y_train.values), cudf.Series(y_valid.values)\n",
    "        \n",
    "        loss = rf_obj(trial, X_train, y_train, X_valid, y_valid)\n",
    "        losses.append(loss)\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(function):\n",
    "    @wraps\n",
    "    def wrapped(*args, **kwargs):\n",
    "        losses = []\n",
    "        for train_index, test_index in k_fold.split(X, y):\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "            X_train, X_valid = cudf.DataFrame.from_pandas(X_train), cudf.DataFrame.from_pandas(X_valid)\n",
    "            y_train, y_valid = cudf.Series(y_train.values), cudf.Series(y_valid.values)\n",
    "            \n",
    "            loss = function(*args, X_train, y_train, X_valid, y_valid, **kwargs)\n",
    "            losses.append(loss)\n",
    "        return np.mean(losses)        \n",
    "    return wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folds(function, X, y, k_fold):\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_valid = cudf.DataFrame.from_pandas(X_train), cudf.DataFrame.from_pandas(X_valid)\n",
    "        y_train, y_valid = cudf.Series(y_train.values), cudf.Series(y_valid.values)\n",
    "        loss = function(X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(trial):\n",
    "    params ={\n",
    "        'myparam': trial.suggest_int('myparam', 1, 10),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def get_model(params):\n",
    "    model = params['myparam']\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside wrap\n",
      "inside w\n",
      "mystring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finished'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w(function):\n",
    "    stringy = 'inside w'\n",
    "    def wrap(*args, **kwargs):\n",
    "        print(\"inside wrap\")\n",
    "        print(stringy)\n",
    "        return function(*args, **kwargs)\n",
    "    return wrap\n",
    "\n",
    "@w\n",
    "def f(mystring):\n",
    "    print(mystring)\n",
    "    return 'finished'\n",
    "\n",
    "f('mystring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 16:24:20,144]\u001b[0m A new study created in memory with name: no-name-6961a733-ee7b-4301-a8f8-aa3d1f0ddb93\u001b[0m\n",
      "\u001b[33m[W 2023-03-15 16:24:20,203]\u001b[0m Trial 0 failed with parameters: {} because of the following error: AttributeError(\"'tuple' object has no attribute 'trial'\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/broug/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_46662/3616520833.py\", line 2, in <lambda>\n",
      "    study.optimize(lambda trial: rf_obj(trial), n_trials=2)\n",
      "  File \"/tmp/ipykernel_46662/1807355700.py\", line 10, in wrapped\n",
      "    loss = function(args.trial, X_train, y_train, X_valid, y_valid)\n",
      "AttributeError: 'tuple' object has no attribute 'trial'\n",
      "\u001b[33m[W 2023-03-15 16:24:20,204]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'trial'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: rf_obj(trial), n_trials\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m rf_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m      4\u001b[0m rf_params\n",
      "File \u001b[0;32m~/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/mambaforge/envs/kaggle/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: rf_obj(trial), n_trials\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m rf_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m      4\u001b[0m rf_params\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36mwrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     X_train, X_valid \u001b[39m=\u001b[39m cudf\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_pandas(X_train), cudf\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_pandas(X_valid)\n\u001b[1;32m      8\u001b[0m     y_train, y_valid \u001b[39m=\u001b[39m cudf\u001b[39m.\u001b[39mSeries(y_train\u001b[39m.\u001b[39mvalues), cudf\u001b[39m.\u001b[39mSeries(y_valid\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m---> 10\u001b[0m     loss \u001b[39m=\u001b[39m function(args\u001b[39m.\u001b[39;49mtrial, X_train, y_train, X_valid, y_valid)\n\u001b[1;32m     11\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(losses)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'trial'"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: rf_obj(trial), n_trials=2)\n",
    "rf_params = study.best_params\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
