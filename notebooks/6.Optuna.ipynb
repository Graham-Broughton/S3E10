{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import optuna\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import catboost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', CFG.NCOLS)\n",
    "pd.set_option('display.max_rows', CFG.NROWS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_eng(df):\n",
    "    df['Skewness_Power3'] = df['Skewness'].apply(lambda x: x**3)\n",
    "    df['EK_Power3'] = df['EK'].apply(lambda x: x**3)\n",
    "    df['cos(EK)'] = df['EK'].apply(lambda x: np.cos(x))\n",
    "    df['SD_x_EK'] = df['SD'] * df['EK']\n",
    "    df['cos(EK)_x_SD'] = df['cos(EK)'] * df['SD']\n",
    "    df['SD_DMSNR_Curve_x_Skewness_Power3'] = df['SD_DMSNR_Curve'] * df['Skewness_Power3']\n",
    "    df['EK_divide_EK_Power3'] = df['EK'] / df['EK_Power3']\n",
    "    df['EK_multiply_SD'] = df['EK'] * df['SD']\n",
    "    df['EK_divide_SD'] = df['EK'] / df['SD']\n",
    "    df['EK_multiply_SD_DMSNR_Curve'] = df['EK'] * df['SD_DMSNR_Curve']\n",
    "    df['SD_DMSNR_Curve_divide_SD'] = df['SD_DMSNR_Curve'] / df['SD']\n",
    "    df['SD_multiply_EK_Power3'] = df['SD'] * df['EK_Power3']\n",
    "    df['SD_DMSNR_Curve_multiply_SD'] = df['SD_DMSNR_Curve'] * df['SD']\n",
    "    df['EK_Power3_multiply_SD_DMSNR_Curve'] = df['EK_Power3'] * df['SD_DMSNR_Curve']\n",
    "    df['cos(EK)_multiply_SD_DMSNR_Curve'] = df['cos(EK)'] * df['SD_DMSNR_Curve']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = f_eng(train)\n",
    "test = f_eng(test)\n",
    "orig = f_eng(orig)\n",
    "\n",
    "y = train['Class']\n",
    "X = train.drop(['Class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_objective(trial):\n",
    "    PATIENCE = 100\n",
    "    depth = trial.suggest_int('depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    l2_leaf_reg = trial.suggest_loguniform('l2_leaf_reg', 1, 100)\n",
    "    random_strength = trial.suggest_loguniform('random_strength', 1, 100)\n",
    "    max_bin = trial.suggest_int('max_bin', 2, 255)\n",
    "    od_wait = trial.suggest_int('od_wait', 10, 100)\n",
    "    grow_policy = trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide'])\n",
    "    \n",
    "    gpu_params = {'task_type' : \"GPU\", 'devices' : '0'}\n",
    "    params = {\n",
    "        'depth': depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'l2_leaf_reg': l2_leaf_reg,\n",
    "        'random_strength': random_strength,\n",
    "        'max_bin': max_bin,\n",
    "        'od_wait': od_wait,\n",
    "        'grow_policy': grow_policy,\n",
    "        }\n",
    "    predsCB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = catboost.CatBoostClassifier(**params, **gpu_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose=False,\n",
    "            )\n",
    "        \n",
    "        predsCB.append(model.predict_proba(X)[:, 1])\n",
    "    resCB = np.mean(predsCB, axis=0)\n",
    "    \n",
    "    return log_loss(y, resCB)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(cb_objective, n_trials=50)\n",
    "CB_params = study.best_params\n",
    "CB_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xb_objective(trial):\n",
    "    PATIENCE = CFG.XB_PATIENCE\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "    subsample = trial.suggest_discrete_uniform('subsample', 0.4, 1.0, 0.1)\n",
    "    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.4, 1.0, 0.1)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 1e-8, 10.0)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 1e-8, 10.0)\n",
    "    \n",
    "    gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "    params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'gamma': gamma,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "    }\n",
    "    \n",
    "    xg_params = {\n",
    "        'n_jobs': -1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'random_state': CFG.SEED\n",
    "    }\n",
    "    \n",
    "    predsXB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = XGBClassifier(**params, **gpu_params, **xg_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose = False\n",
    "            )\n",
    "        \n",
    "        predsXB.append(model.predict_proba(X)[:, 1])\n",
    "    resXB = np.mean(predsXB, axis=0)\n",
    "    \n",
    "    return log_loss(y_valid, resXB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(xb_objective, n_trials=50)\n",
    "xgb_params = study.best_params\n",
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lb_objective(trial):\n",
    "    PATIENCE = CFG.XB_PATIENCE\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 7)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 0.3)\n",
    "    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n",
    "    subsample = trial.suggest_discrete_uniform('subsample', 0.4, 1.0, 0.1)\n",
    "    colsample_bytree = trial.suggest_discrete_uniform('colsample_bytree', 0.4, 1.0, 0.1)\n",
    "    reg_alpha = trial.suggest_loguniform('reg_alpha', 1e-8, 10.0)\n",
    "    reg_lambda = trial.suggest_loguniform('reg_lambda', 1e-8, 10.0)\n",
    "    \n",
    "    # gpu_params = {'tree_method' : \"gpu_hist\", 'gpu_id' : 0}\n",
    "    params = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'reg_alpha': reg_alpha,\n",
    "        'reg_lambda': reg_lambda,\n",
    "    }\n",
    "\n",
    "    lg_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'logloss',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': CFG.SEED\n",
    "    }\n",
    "    \n",
    "    k_fold = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=CFG.SEED) \n",
    "    predsLB = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model = lgbm.LGBMClassifier(**params, **lg_params)\n",
    "        \n",
    "        model.fit(X=X_train, y=y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            eval_metric = 'logloss',\n",
    "            early_stopping_rounds = PATIENCE,\n",
    "            verbose = False\n",
    "            )\n",
    "        \n",
    "        predsLB.append(model.predict_proba(X_valid)[:, 1])\n",
    "    resLB = np.mean(predsLB, axis=0)\n",
    "    \n",
    "    return log_loss(y_valid, resLB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lb_objective, n_trials=50)\n",
    "lb_params = study.best_params\n",
    "lb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(X_tr, y_tr, X_val, y_val, params, r_params):\n",
    "    cX = cudf.DataFrame.from_pandas(X)\n",
    "    cy = cudf.Series(y.values)\n",
    "\n",
    "    \n",
    "    model = curfc(**params, **r_params)\n",
    "    \n",
    "    model.fit(X=X_tr, y=y_tr)\n",
    "    preds = model.predict_proba(X_val)\n",
    "    loss = log_loss(y_val, preds)\n",
    "    return loss\n",
    "\n",
    "def rf_obj(trial, X_tr, y_tr, X_val, y_val):\n",
    "    n_bins = trial.suggest_int('n_bins', 8, 256)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 10000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 10)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    max_features = trial.suggest_uniform('max_features', 0.1, 1.0)\n",
    "    max_leaves = trial.suggest_int('max_leaf_nodes', 2, 30)\n",
    "    min_impurity_decrease = trial.suggest_uniform('min_impurity_decrease', 0.0, 0.5)\n",
    "    max_samples = trial.suggest_uniform('max_samples', 0.1, 1.0)\n",
    "    \n",
    "    params = {\n",
    "        'n_bins': n_bins,\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'max_features': max_features,\n",
    "        'max_leaves': max_leaves,\n",
    "        'min_impurity_decrease': min_impurity_decrease,\n",
    "        'max_samples': max_samples,\n",
    "    }\n",
    "    r_params = {\n",
    "        'random_state': CFG.SEED,\n",
    "    }\n",
    "    \n",
    "    preds = rf(X_tr, y_tr, X_val, y_val, params, r_params)\n",
    "    return preds\n",
    "\n",
    "def rf_objective(trial, X, y):\n",
    "    \n",
    "    losses = []\n",
    "    for train_index, test_index in k_fold.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_valid = cudf.DataFrame.from_pandas(X_train), cudf.DataFrame.from_pandas(X_valid)\n",
    "        y_train, y_valid = cudf.Series(y_train.values), cudf.Series(y_valid.values)\n",
    "        \n",
    "        loss = rf_obj(trial, X_train, y_train, X_valid, y_valid)\n",
    "        losses.append(loss)\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: rf_objective(trial, X, y), n_trials=2)\n",
    "rf_params = study.best_params\n",
    "rf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
