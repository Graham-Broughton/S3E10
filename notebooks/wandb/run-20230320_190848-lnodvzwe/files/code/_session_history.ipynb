{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d28623df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48bba265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84025809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cabb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736a330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d72395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf47af93",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7d71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "832e3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fe327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "595ee05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1f3a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d10bfd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab7afccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('..')\n",
    "from config import CFG\n",
    "from src.model.include_concat import NN\n",
    "CFG = CFG()\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0f17d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Style, Fore\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "mgt = Style.BRIGHT + Fore.MAGENTA\n",
    "grn = Style.BRIGHT + Fore.GREEN\n",
    "gld = Style.BRIGHT + Fore.YELLOW\n",
    "res = Style.RESET_ALL\n",
    "\n",
    "rc = {\n",
    "    \"axes.facecolor\": \"#FFFEF8\",\n",
    "    \"figure.facecolor\": \"#FFFEF8\",\n",
    "    \"axes.edgecolor\": \"#000000\",\n",
    "    \"grid.color\": \"#EBEBE7\" + \"30\",\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelcolor\": \"#000000\",\n",
    "    \"xtick.color\": \"#000000\",\n",
    "    \"ytick.color\": \"#000000\",\n",
    "    \"grid.alpha\": 0.4\n",
    "}\n",
    "sns.set(rc=rc)\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02408562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_(column):\n",
    "    return np.log(-min(column) + 1 + column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ebc7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "336e604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1).values\n",
    "y = train['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c2ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b0685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0791e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b797fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(train)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31838bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=5, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "845e2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "for train_idx, val_idx in k_fold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        X_train, y_train, \n",
    "        batch_size=CFG.BATCH_SIZE, \n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=(X_val, y_val),\n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(X_train)\n",
    "    train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_loss)\n",
    "\n",
    "    oof_preds = ensemble.predict(X_val)\n",
    "    oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a7cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X_data,y_data,n_splits):\n",
    "\n",
    "    def gen():\n",
    "        for train_index, test_index in KFold(n_splits).split(X_data):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,y_train,X_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))\n",
    "\n",
    "dataset=make_dataset(X,y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5b0b99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None), TensorSpec(shape=<unknown>, dtype=tf.float64, name=None))>"
     ]
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1e2bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c47edbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bbff4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, next(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46fc6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, item in enumerate(iter(dataset)):\n",
    "    print(num, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f39a0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf1f0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.shard(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e06763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59695ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33286634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2c1dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(combinations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3a72d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "arr = ['ds1', 'ds2', 'ds3', 'ds4', 'ds5']\n",
    "print(list(permutations(arr, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a51a528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49223748",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "847ac8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    print(train)\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3452ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val = ds\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    it = iter(train)\n",
    "    x = next(it)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f612477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = (x, y for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01a470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c2302ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(-1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67a40caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = ((x, y) for x, y in train_ds.take(1))\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8a5f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate((x, y) for x, y in train_ds)\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00632680",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x, y = np.concatenate([(x, y) for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d828884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.to_numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff95c432",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10de02e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fe1012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(-1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd096652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)], axis=0)\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0240a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds.take(1)])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2ed0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "406f6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "x = np.concatenate([x for x, y in train_ds])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45568fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShardDataset element_spec=(TensorSpec(shape=(8,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1061c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0ea7b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 8), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
     ]
    }
   ],
   "source": [
    "val_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a81b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a061ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds.take(1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fe95e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "506487b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb43b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "63cfbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds.batch(64)\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b4c85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*val_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "344e243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a7b0ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "469ac46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9658476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "ds = arr[-1]\n",
    "val_ds = ds\n",
    "dslist = [d for d in arr if d is not ds]\n",
    "train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "train_ds = train_ds.shuffle(len(y)*2)\n",
    "data, labels = tuple(zip(*train_ds))\n",
    "\n",
    "x = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "570e3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8ce8f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "22819d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(os.path.join(CFG.RAW_DATA, 'Pulsar.csv'))\n",
    "train = pd.read_csv(os.path.join(CFG.RAW_DATA, 'train.csv')).drop(columns='id')\n",
    "test = pd.read_csv(os.path.join(CFG.RAW_DATA, 'test.csv')).drop(columns='id')\n",
    "\n",
    "train[['Skewness', 'Skewness_DMSNR_Curve']] = train.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})\n",
    "test[['Skewness', 'Skewness_DMSNR_Curve']] = test.apply({'Skewness': log_, 'Skewness_DMSNR_Curve': log_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d6197a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Class', axis=1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b580c102",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    #WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4323ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "#wandb.init(project='S3E10', group='NN', name='NN', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e00ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, mode='min', restore_best_weights=True, verbose=2\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, mode='min', restore_best_weights=True, min_lr=1e-12, verbose=2\n",
    "    ),\n",
    "    WandbMetricsLogger()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "549b0817",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds #.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79df1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NNtest', name='NNtest', config=params)\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc896ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = NN(test)\n",
    "ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params, tags=['CV5'])\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b436ab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2p3cey6z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e3afe00b2549be933fb993bd0d5d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.043 MB of 0.043 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/binary_accuracy</td><td></td></tr><tr><td>epoch/epoch</td><td></td></tr><tr><td>epoch/learning_rate</td><td></td></tr><tr><td>epoch/loss</td><td></td></tr><tr><td>epoch/lr</td><td></td></tr><tr><td>epoch/val_binary_accuracy</td><td></td></tr><tr><td>epoch/val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/binary_accuracy</td><td>0.98755</td></tr><tr><td>epoch/epoch</td><td>2</td></tr><tr><td>epoch/learning_rate</td><td>0.001</td></tr><tr><td>epoch/loss</td><td>0.04491</td></tr><tr><td>epoch/lr</td><td>0.001</td></tr><tr><td>epoch/val_binary_accuracy</td><td>0.98835</td></tr><tr><td>epoch/val_loss</td><td>0.042</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">NN</strong> at: <a href='https://wandb.ai/g-broughton/S3E10/runs/2p3cey6z' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/2p3cey6z</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230320_190630-2p3cey6z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2p3cey6z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae88961918b42d79a2c3629b5b2548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668729266530135, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/broug/Desktop/S3E10/notebooks/wandb/run-20230320_190848-lnodvzwe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-broughton/S3E10/runs/lnodvzwe' target=\"_blank\">NN</a></strong> to <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-broughton/S3E10' target=\"_blank\">https://wandb.ai/g-broughton/S3E10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-broughton/S3E10/runs/lnodvzwe' target=\"_blank\">https://wandb.ai/g-broughton/S3E10/runs/lnodvzwe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params, tags=['CV5'])\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    ensemble = NN(test)\n",
    "    ensemble.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    \n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    # train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    # train_log_loss.append(train_loss)\n",
    "\n",
    "    # oof_preds = ensemble.predict(X_val)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    # oof_log_loss.append(oof_loss)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "338526d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_fold = RepeatedStratifiedKFold(n_splits=CFG.NFOLDS, n_repeats=CFG.REPEATS, random_state=CFG.SEED)\n",
    "\n",
    "params = {\n",
    "    'folds': CFG.NFOLDS,\n",
    "    'repeats': CFG.REPEATS,\n",
    "    'batch_size': CFG.BATCH_SIZE,\n",
    "    'learning_rate': CFG.LR,\n",
    "}\n",
    "wandb.init(project='S3E10', group='NN', name='NN', config=params, tags=['CV5'])\n",
    "\n",
    "train_log_loss = []\n",
    "oof_log_loss = []\n",
    "models = []\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(len(y)*2)\n",
    "ds1 = ds.shard(5, 0)\n",
    "ds2 = ds.shard(5, 1)\n",
    "ds3 = ds.shard(5, 2)\n",
    "ds4 = ds.shard(5, 3)\n",
    "ds5 = ds.shard(5, 4)\n",
    "\n",
    "arr = [ds1, ds2, ds3, ds4, ds5]\n",
    "\n",
    "for ds in arr:\n",
    "    ensemble = NN(test)\n",
    "    ensemble.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=CFG.LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    \n",
    "    val_ds = ds.batch(64)\n",
    "    dslist = [d for d in arr if d is not ds]\n",
    "    train_ds = dslist[0].concatenate(dslist[1]).concatenate(dslist[2]).concatenate(dslist[3])\n",
    "    train_ds = train_ds.shuffle(len(y)*2).batch(128)\n",
    "\n",
    "    history = ensemble.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.EPOCHS, callbacks=callbacks, \n",
    "        validation_data=val_ds, \n",
    "        use_multiprocessing=True\n",
    "    )\n",
    "\n",
    "    train_preds = ensemble.predict(train_ds)\n",
    "    # train_loss = log_loss(y_train, train_preds)\n",
    "    train_log_loss.append(train_preds)\n",
    "\n",
    "    oof_preds = ensemble.predict(val_ds)\n",
    "    # oof_loss = log_loss(y_val, oof_preds)\n",
    "    oof_log_loss.append(oof_preds)\n",
    "\n",
    "    models.append(ensemble)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
